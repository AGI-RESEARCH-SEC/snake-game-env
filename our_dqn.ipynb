{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation from 'S' to 'D': (0.3, 0.78)\n",
      "Euclidean distance between 'S' and 'D': 0.4307516219945733\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Bridge:\n",
    "    '''\n",
    "        Bridges between the snake_environment and DQN agent.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def min_max_normalize(value, min_value, max_value):\n",
    "            return (value - min_value) / (max_value - min_value)\n",
    "    @staticmethod\n",
    "    def euclidean_distance(source, destination):\n",
    "        distance = math.sqrt((source[0] - destination[0]) ** 2 + (source[1] - destination[1]) ** 2)\n",
    "        normalized_distance = Bridge.min_max_normalize(distance, 0, 39.941)\n",
    "        return normalized_distance\n",
    "\n",
    "    @staticmethod\n",
    "    def find_position(maze, element):\n",
    "        \"\"\"Find the position of a given element in the maze.\"\"\"\n",
    "        for i, row in enumerate(maze):\n",
    "            for j, val in enumerate(row):\n",
    "                if val == element:\n",
    "                    return i, j\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_representation(maze, source_element='S', destination_element='D'):\n",
    "        \"\"\"Calculate and normalize the vector representation from source to destination.\"\"\"\n",
    "\n",
    "        source_pos = Bridge.find_position(maze, source_element)\n",
    "        destination_pos = Bridge.find_position(maze, destination_element)\n",
    "\n",
    "        if source_pos and destination_pos:\n",
    "            vector_x = destination_pos[1] - source_pos[1]  # x-component (column difference)\n",
    "            vector_y = destination_pos[0] - source_pos[0]  # y-component (row difference)\n",
    "\n",
    "            # Normalize vector_x and vector_y\n",
    "            normalized_vector_x = Bridge.min_max_normalize(vector_x, -25, 25)\n",
    "            normalized_vector_y = Bridge.min_max_normalize(vector_y, -25, 25)\n",
    "\n",
    "            return (normalized_vector_x, normalized_vector_y)\n",
    "        else:\n",
    "            return None\n",
    "    @staticmethod\n",
    "    def get_state(env):\n",
    "        # Get the current state of the environment\n",
    "        vector = Bridge.vector_representation(maze)\n",
    "\n",
    "        # Calculate the euclidean distance between source and destination\n",
    "        source_pos = Bridge.find_position(maze, 'S')\n",
    "        destination_pos = Bridge.find_position(maze, 'D')\n",
    "        distance = Bridge.euclidean_distance(source_pos, destination_pos)\n",
    "\n",
    "        # state\n",
    "        state = torch.tensor([vector[0], vector[1], distance])\n",
    "        return state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    maze = [\n",
    "            ['0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '0'],\n",
    "            ['1', '1', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0'],\n",
    "            ['0', '1', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1', '0'],\n",
    "            ['0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '1'],\n",
    "            ['0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0'],\n",
    "            ['0', '1', '0', '0', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0'],\n",
    "            ['0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '0', '0'],\n",
    "            ['0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '1', '0', '1', '1', '0'],\n",
    "            ['0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0'],\n",
    "            ['1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '1', '0'],\n",
    "            ['0', '0', '0', '0', '1', '0', '0', '1', '0', '1', 'S', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0', '0'],\n",
    "            ['0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0'],\n",
    "            ['1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0'],\n",
    "            ['1', '0', '1', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '0'],\n",
    "            ['0', '1', '0', '1', '1', '0', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0'],\n",
    "            ['0', '0', '0', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0'],\n",
    "            ['1', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0'],\n",
    "            ['0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '0'],\n",
    "            ['0', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
    "            ['1', '0', '1', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0'],\n",
    "            ['0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0'],\n",
    "            ['1', '1', '1', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0'],\n",
    "            ['0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '0'],\n",
    "            ['0', '1', '1', '0', '0', '1', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0'],\n",
    "            ['D', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0']\n",
    "       ]\n",
    "    \n",
    "    vector = Bridge.vector_representation(maze)\n",
    "    print(\"Vector representation from 'S' to 'D':\", vector)\n",
    "\n",
    "    # Calculate the euclidean distance between source and destination\n",
    "    source_pos = Bridge.find_position(maze, 'S')\n",
    "    destination_pos = Bridge.find_position(maze, 'D')\n",
    "    distance = Bridge.euclidean_distance(source_pos, destination_pos)\n",
    "    print(\"Euclidean distance between 'S' and 'D':\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/anon/reso/major_project/snake-game/dqn/our_dqn.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anon/reso/major_project/snake-game/dqn/our_dqn.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anon/reso/major_project/snake-game/dqn/our_dqn.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anon/reso/major_project/snake-game/dqn/our_dqn.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)  # Input Layer\n",
    "        self.fc2 = nn.Linear(24, 24)          # Hidden Layer\n",
    "        self.fc3 = nn.Linear(24, action_size) # Output Layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Linear output for action values\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_model(state_size, action_size):\n",
    "        # Creating the model instance\n",
    "        dqn_model = DQN(state_size, action_size)\n",
    "\n",
    "        # Defining optimizer and loss function\n",
    "        optimizer = optim.Adam(dqn_model.parameters())  # Adam optimizer\n",
    "        loss_function = nn.MSELoss()                    # Mean Squared Error Loss\n",
    "        \n",
    "        return dqn_model, optimizer, loss_function\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_action(model, state):\n",
    "        # vector, normalized_distance = state\n",
    "\n",
    "        # Convert the inputs to a torch tensor in the same format as the training data\n",
    "        # model_input = torch.tensor([vector[0], vector[1], normalized_distance]).float()\n",
    "        model_input = torch.tensor([state[0], state[1], state[2]]).float()\n",
    "\n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():  # We don't need to track gradients here\n",
    "            model_output = model(model_input)\n",
    "\n",
    "        # Get the action with the highest Q-value\n",
    "        best_action = torch.argmax(model_output).item()\n",
    "        \n",
    "        return best_action\n",
    "\n",
    "# Example usage\n",
    "vector_size = 2  # x, y components of the vector\n",
    "distance_size = 1  # normalized distance\n",
    "\n",
    "state_size = vector_size + distance_size\n",
    "action_size = 4  # four possible directions to move\n",
    "\n",
    "model, optimizer, loss_fn = DQN.build_model(state_size, action_size)\n",
    "\n",
    "# ---------------------------------\n",
    "# Training the model on dummy data\n",
    "# ---------------------------------\n",
    "\n",
    "# Dummy data generation\n",
    "num_samples = 1000\n",
    "dummy_inputs = torch.rand(num_samples, vector_size + distance_size)\n",
    "dummy_outputs = torch.randint(0, action_size, (num_samples,))\n",
    "dummy_outputs = nn.functional.one_hot(dummy_outputs, num_classes=action_size).float()\n",
    "\n",
    "print(f'Input shape: {dummy_inputs.shape} \\n input sample: {dummy_inputs[0]}')      # Input shape: torch.Size([1000, 3])     input sample:  tensor([0.1270, 0.6738, 0.6910])\n",
    "print(f'Output shape: {dummy_outputs.shape} \\n output sample: {dummy_outputs[0]}') # Output shape: torch.Size([1000, 4])     output sample: tensor([1., 0., 0., 0.])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_samples):\n",
    "        # Forward pass\n",
    "        outputs = model(dummy_inputs[i])\n",
    "        loss = loss_fn(outputs, dummy_outputs[i])\n",
    "\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()   # zero the gradients because they accumulate by default\n",
    "        loss.backward()         # calculate the gradients\n",
    "        optimizer.step()        # update the parameters\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# prediction\n",
    "sample_input = torch.rand(1, vector_size + distance_size)\n",
    "sample_output = model(sample_input)\n",
    "print(f'Input: {sample_input} \\n Output: {sample_output}')\n",
    "\n",
    "# Example usage\n",
    "vector_example = (0.5, -0.3)  # Example vector representation\n",
    "normalized_distance_example = 0.8  # Example normalized distance\n",
    "\n",
    "# Predict the action\n",
    "predicted_action = DQN.predict_action(model, dummy_inputs[0])\n",
    "print(f\"Predicted action: {predicted_action} \\n Expected action: {torch.argmax(dummy_outputs[0]).item()}\")\n",
    "'''\n",
    "note:\n",
    "    torch.argmax: index of max value\n",
    "    .item(): value of the tensor\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    ['S', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '0'],\n",
    "    ['1', '1', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0'],\n",
    "    ['0', '1', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1', '0'],\n",
    "    ['0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '1'],\n",
    "    ['0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0'],\n",
    "    ['0', '1', '0', '0', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0'],\n",
    "    ['0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '0', '0'],\n",
    "    ['0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '1', '0', '1', '1', '0'],\n",
    "    ['0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0'],\n",
    "    ['1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '1', '0'],\n",
    "    ['0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0', '0'],\n",
    "    ['0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0'],\n",
    "    ['1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0'],\n",
    "    ['1', '0', '1', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '0'],\n",
    "    ['0', '1', '0', '1', '1', '0', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0'],\n",
    "    ['0', '0', '0', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0'],\n",
    "    ['1', '0', '1', '0', '1', 'F', '0', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0'],\n",
    "    ['0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '0'],\n",
    "    ['0', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
    "    ['1', '0', '1', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0'],\n",
    "    ['0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0'],\n",
    "    ['1', '1', '1', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0'],\n",
    "    ['0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '0'],\n",
    "    ['0', '1', '1', '0', '0', '1', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0'],\n",
    "    ['0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DQNAgent class\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from collections import deque\n",
    "\n",
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size    # we need it to define the input layer of the network\n",
    "        self.action_size = action_size  # we need it to define the output layer of the network\n",
    "        self.memory = deque(maxlen=2000)    # deque is a list-like container with fast appends and pops on either end\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model, self.optimizer, self.loss_function = DQN.build_model(state_size=3, action_size=4)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = DQN.predict_action(self.model, state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = np.array(random.sample(self.memory, batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)    # target_f is the predicted Q values\n",
    "            target_f[0][action] = target            # target_f[0][action] is the Q value of the action taken\n",
    "            '''\n",
    "            note: target_f[0] : keras model returns a list of lists, so we need to access the first element of the list to get the predicted Q values\n",
    "            '''\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)    # re-training the model\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)  # get random batch (i.e. batch with random elements) from memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            target = reward + (self.gamma * torch.max(self.model(next_state)).item()) * (not done)  # Bellman equation (discounted reward)\n",
    "            '''\n",
    "                note: *(not done): gives 0 if done is True, 1 otherwise\n",
    "                target is target Q value\n",
    "            '''\n",
    "\n",
    "            # Predicted Q values\n",
    "            pred_q_values = self.model(state)   # predicted Q values for the current state\n",
    "            target_q_values = pred_q_values.clone().detach()    # clone the predicted Q values\n",
    "            target_q_values[action] = target                    # update the Q value of the action taken\n",
    "\n",
    "            # Back-propagate and optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            '''\n",
    "                Before you start a new optimization step, you need to zero out the previously accumulated gradients.\n",
    "                clears old gradients from the last step (if they exist).\n",
    "            '''\n",
    "            loss = self.loss_function(pred_q_values, target_q_values)   # calculate the loss\n",
    "            loss.backward()         # backward pass. \n",
    "            '''\n",
    "                note: pred_q_values remembers the graph of operations that created it thats how it knows which model to backpropagate\n",
    "            '''\n",
    "            self.optimizer.step()   # update the model parameters\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay  # decrease the exploration rate\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# To Do: Update below code to use PyTorch  + Snake environment\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "from snake_maze_env import MazeSnakeGameEnv\n",
    "# Create the environment\n",
    "env = MazeSnakeGameEnv(maze, height=25, width=25, snake_growth = False, boundary_loop = False)\n",
    "state_size = 3\n",
    "action_size = 4\n",
    "\n",
    "# Initialize the DQN agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "batch_size = 32\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # updated maze\n",
    "    # state = np.reshape(state, [1, state_size])  # Reshape the state to a 1x3 vector so that it can be fed to the network\n",
    "    dqn_state = Bridge.get_state(env)\n",
    "    for t in range(500):\n",
    "        # Render the environment (optional)\n",
    "        env.render()\n",
    "\n",
    "        # Choose an action\n",
    "        dqn_action = agent.act(dqn_state)\n",
    "\n",
    "        # Perform the action\n",
    "        next_state, reward, done, _ = env.step(dqn_action)\n",
    "        # next_state = np.reshape(next_state, [1, state_size])\n",
    "        next_dqn_state = Bridge.get_state(env)\n",
    "\n",
    "        # Remember the experience\n",
    "        agent.remember(state, action, reward, next_dqn_state, done)\n",
    "\n",
    "        # Update the state\n",
    "        dqn_state = next_dqn_state\n",
    "\n",
    "        # Check if episode is finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Train the agent\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions\n",
    "* update rewards from replay memory before training? discounting reward as agent is farther from target?\n",
    "- yes update using discount factor\n",
    "\n",
    "# References\n",
    "* [colab code](https://colab.research.google.com/drive/?authuser=2#create=1&folderId=1kvo56FfBy3KNHKiRdLoUsl5KlScGJ_21)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
