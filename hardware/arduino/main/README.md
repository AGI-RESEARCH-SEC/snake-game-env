## Arduino Q-Learning implementation
* We trained RL agent using Q-Learning [here](https://github.com/AGI-RESEARCH-SEC/snake-game-env/blob/main/q_learning.ipynb)
* Then flattened the weights and used those weights to make decisions.

## Maze-1
```
maze = np.array([
    [1, 1, 0, 1],
    [1, 1, 0, 0],
    [1, 1, 0, 1],
    [1, 0, 0, 1],
    [1, 0, 0, 0]
])
source = (1,3)
destination = (4,3)
facing = "left"
```
**Flattened-Weights-1**: `[-0.14567542786524684, 0.028992119894766844, -0.2658059392757915, 15.500593710459835, -0.15679, -0.5751383360655641, -0.5, -0.7610600150158306, 19.256544264254092, 16.330889837828664, -2.594840781380889, -2.474224227054556, -0.852180077055561, -0.7006457235470309, 12.950534339413846, -2.2842125693118476, -2.776785077874974, -2.6859971955040907, -1.6150456834794744, -1.8485000844551536, 12.096579828480882, -2.7116605133971876, -3.133167873327339, -3.182911142097555]
`
## Maze-2: Obstacle Avoidance
```
maze = np.array([
    [1, 1, 1, 1],
    [1, 1, 0, 1],
    [1, 0, 0, 1],
    [1, 0, 1, 1],
    [1, 0, 0, 1],
    [1, 1, 0, 0],
    [1, 1, 1, 1]
])
source = (1,2)
destination = (5,3)
facing = "down"
```
**Flattened-Weights-2**[0.0, 0.0, 0.0, -8.038707105143146, -8.082612249219368, -8.075357976574818, 0.0, 0.0, 0.0, -6.647911287348904, -8.350614717222712, -8.359131172256962, 0.0, 0.0, 0.0, -8.401727003925874, -7.051792274204917, -8.407046054870735, -8.05865436530638, -8.055914330703747, -7.195154863191697, -8.159045656801942, -8.353843068419978, -8.358464332980923]

